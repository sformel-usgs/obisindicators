---
title: "Examining Global Biodiversity with OBIS and GBIF Data"
author: "Stephen Formel"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true


---

```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 300px;
}
```

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>",
  warning  = FALSE, 
  message  = FALSE,
  out.width = "90%")
```

## What are we doing? 

OBIS is small enough to analyse by itself on a decent laptop computer or a small cluster. We have previously visualized ES50 for OBIS data by decade.

<img src="../../static/img/decadal_animation.gif" alt="OBIS Global ES50" width="90%"/>

But adding GBIF has an order of magnitude more records than OBIS, so combining the two takes it to another level.  Options for tackling this volume of data include increasing the memory in your cluster or splitting up the analysis into more manageable chunks.  Here we demonstrate the latter.

## Summary of Exploratory Analysis

I had to make the analysis work on one of three resources:

1. My government laptop:
   - Has 12th Gen Intel(R) Core(TM) i7-1265U   1.80 GHz with 32 GB of RAM
   
1. An AWS cloud instance with 8 cpu and 32 GB of RAM

1. The USGS HPC Denali
   - A single node has Two 20-core 2.4 Ghz Intel Xeon Gold 6148 processors (Skylake) with 192 GB RAM (4.8 GB/core)

I ended up using the last option. Either of the other two were able to handle smaller subsets of the data, like the continental USA, but couldn't handle all quadrants of the globe for the full 2010s.

### arrow, dask and spark are not enough

The data are ~ 185 GB, representing 2.4 billion records and XX columns.  The parquet file format helps, but generally chokes when trying to calculate the diversity metrics because:

1. The biodiversity indices are not simple summary statistics, like mean.
1. The data needs to be grouped by h3 cells. At high resolutions, this means there are millions of cells, and the parquet file is not partitioned by h3 cell.  This means you either need a large amount of memory to hold the entire dataset while you compute the indices, or you need to repartition the data after indexing by cell, so that the indices can be calculated in subsets of cells.

As the [dask documentation](https://docs.dask.org/en/stable/best-practices.html) points out, sometimes you need to chunk up your data. I tried using `dask` and `spark` in addition to R `arrow` but stopped putting effort into those methods because I was less comfortable with python and java then R and it became apparent that no matter what I would have to process the data in chunks to compute the global data set on the resources I had access to. The R package `sparklyr` required installation of a Spark environment, something I couldn't do without IT intervention, so I didn't pursue it. Similarly, while it was easy to spin up a dask cluster on AWS, I couldn't easily do that on our HPC, so I didn't put effort into pursuing it. But going forward, both options could probably accomplish the same thing as this R work flow.

### binned values are easier to interpret than continuous

We created maps with both continuous color schemes and with binned color schemes.  We found that in the former it was difficult to differentiate the middle values.  Our conclusion was that precise values were not as useful as being able to quickly understand where biodiversity was extremely low, high, and what the approximate "normal" was for various regions.

|Continuous Color | Discrete Color |
|----|----|
|<img src="../output/tests/ES100_CONUS/ES100_CONUS_2010_res_5.png" alt="Continuous color scale - ES100_CONUS_2010_res_5" width="95%"/>|<img src="../output/tests/ES100_CONUS/ES100_CONUS_2010_res_5_discrete.png" alt="Discrete color scale - ES100_CONUS_2010_res_5" width="95%"/>|


### Hurlbert Index (ESi) vs Traditional Biodiversity metrics

See [my explanation of ESi](./ES50_explained.Rmd) for background on the metric.

We examined the relationship between ESi, Richness, Shannon Diversity, and Hill Number of 1.  Ultimately, it was clear that there was a strong correlation between Shannon Diversity and ESi for this set of data. In the image below, the panels represent different h3 resolutions.  There didn't seem to be a strong influence of resolution on the relationship, or either metric. 

<img src="../sformel_notebooks/../output/tests/comparison_of_indices/ESvShannon.png" alt="Correlation between ESi and Shannon at different resolutions" width="50%"/>

This is probably because:

1. We aren't yet accounting for abundance beyond each occurrence record (e.g. individualCount or organismQuantity)
1. There are some strong sampling biases in the GBIF and OBIS data.  This is explained a bit in [John Waller's blog post](https://data-blog.gbif.org/post/exploring-es50-for-gbif/#es50-fail-cases).

However, I think it's arguable that the output of ESi is more intuitive to humans than Shannon Diversity while avoiding the extreme effects of sampling bias that are associated with richness.

### ES50 vs ES100

We weren't sure what the best sampling depth was for ESi for these global datasets.  We wanted to achieve something that could resolve real differences in diversity, while discarding as few cells as possible for lack of data.  Some past studies had looked at ES50 and ES100, so we did the same. We found that ES100 worked really well for areas like the United States where there is an abundance data.  ES50 didn't provide as much nuance, but was able to provide values for more cells.

|ES50 | ES100 |
|----|----|
|<img src="../output/tests/ES50_CONUS/ES50_CONUS_1960-2022_res_5_discrete.png" alt="Continuous color scale - ES50_CONUS_2010_res_5" width="95%"/>|<img src="../output/tests/ES100_CONUS/ES100_CONUS_2010_res_5_discrete.png" alt="Discrete color scale - ES50_CONUS_2010_res_5" width="95%"/>|

After quality filtering (we still haven't addressed "issue" flags), we had data in 533564 of 2016842 cells, or about 1/4 of the cells on the globe.  Of these, we could calculate a non-NA value for about 30% of them when using ES50 (so about 8% of the total cells on the globe) vs 23% with ES100 (about 6% of the total cells on the globe).  

### Using the h3 grid: pros and cons

We struggled with consistently implementing the dggridR system.  We had too many collaborators struggle with installation to make it seem like a good system with regard to reproduciblity. The [h3 system](https://www.uber.com/blog/h3/) is a nice grid solution for a variety of reasons, and we were intrigued by the nesting properties, which have proven to be useful for exploring this data and our analysis directions.

However, hexagons are more computationally costly than rectangles and represent a hurdle in generating an sharing the data. Furthermore, the hexagons in the h3 system vary in area relative to the icosahedron vertices, so they are not as statistically rigorous as the dggridr system.

#### resolution with regard to ESi

If we use a coarser resolution, than we can reduce the number of NA values with ESi.  This is very pleasing to look at, but paints some polities in too broad of strokes in addition to giving the illusion that our data covers the earth much more completely that is true. For example, at a resolution of 3, 20% of the [countries in the world](https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_area) are represented by ~ 1 hexagon, or less. We chose to work with a h3 resolution of 5. This resolution means each hexagon is ~ 253 km^2, [on average](https://h3geo.org/docs/core-library/restable/#average-area-in-km2). We chose this after examining some sample data at resolutions 2 - 7.  We felt a resolution of 5 balanced our desire to show hi-resolution diversity metrics without discarding too much data that was too sparse to meet the requirments of the Hurlbert (AKA ES) index). For perspective, at this resolution, the Great Barrier Reef contains about 1375 hexagons and the island country of Barbados would be covered by about 2 hexagons.    

## Strategy for analysis

We're going to visualize our biodiversity values as h3 hexagons and by decade, since these are two useful units for aggregating the data into a meaningful amount of time and space.  So our strategy will be to:

1. Remove data that isn't needed.
1. Index each occurrence record to an h3 cell.
1. Divide the earth into four quadrants and index the h3 cell centroids to those quadrants.
1. Write new parquet files, re-partitioned by year and quadrant. These parquet files are our new starting point.
1. Next we will:
   1. read in the parquet files
   1. join the OBIS and GBIF data
   1. calculate the diversity indices for each cell by decade
   1. join with the h3 grid and visualize.

## Download and clean data

It is much faster to work with these large parquet files locally than calling an S3 bucket via `arrow` or another package.  Every month, GBIF publishes a snapshot of their database via [AWS](https://registry.opendata.aws/gbif/).  The OBIS snapshot is published every **XXXX** and can be found at https://obis.org/data/access/#

Download these parquet files to your local machine as the raw data for this analysis.

### Remove data that isn't needed.

In addition to removing undeeded columns, we'll only work with the most recent decade of data for this vignette.  A few caveats about the code:

1. We're doing some filtering based on recommendations from other GBIF/OBIS users (linked below). That being said, I haven't yet dealt with the "issues" column, as recommended, because the array format was difficult to deal with before the "collect" function.  Eventually, issues will be filtered out too.

   1. [Data Use Club](https://www.gbif.org/event/2CAcHI4oxVK5ZgMnFszNUD/data-use-club-practical-sessions-data-quality)
   1. [Mark Costello Blog Post](https://www.oceansofbiodiversity.auckland.ac.nz/2018/09/29/tips-when-selecting-biodiversity-data-from-gbif-and-obis/)
1. As fantastic as the R `arrow` package is, it doesn't have the full dplyr function implemented yet. For example `%in%` and `!=` aren't implemented yet in filter arrow. So you'll see a few places where the code is more verbose than it typically needs to be for filtering the data.

### Process OBIS

```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('../src/R/HPC/obis_download_and_reduce_parquet_global.R')
```

```{r, OBIS Download and Clean, echo=TRUE, eval=FALSE}
```


Now we need to make the h3 and quadrant indices so we can apply them to our occurrence records. First index the h3 cells to quadrants...

```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('../src/R/HPC/h3_index_by_quadrant.R')
```

```{r, Index h3 to quadrant, echo=TRUE, eval=FALSE}
```

...then index the OBIS records to both. 

```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('../src/R/HPC/obis_download_and_reduce_parquet_global.R')
```


```{r, OBIS Index records to h3 and quadrants, echo=TRUE, eval=FALSE}
```


Write the object to parquet files, optimizing future analysis by partitioning by year and quadrant. 

```{r OBIS write parquet files, echo=TRUE, eval = FALSE}
```

### Process GBIF

This is very similar to the OBIS cleaning above.  Mostly there are only differences in the capitalization of column names (why?)  Since we have already created quadrant indices, we don't need to do that again. One big difference is that the GBIF data is big enough that the resources we had on hand weren't sufficient to process 2010 - 2019 in one go.  So instead we loop through each partition. This took about 1-3 seconds for each of the partitions, so about 50 minutes total.

```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('../src/R/HPC/gbif_download_and_reduce_parquet_global.R')
```

```{r GBIF download and process, echo=TRUE, eval = FALSE}
```



## Create Diversity Indices & h3 Grid

We will use a modified version of the `obisindicators` function, `obis_calc_indicators`, to calculate several indices at the same time.  Run `?obisindicators::calc_indicators` to learn more about the input and output structure.

I found that the function currently in the `obisindicators` package is a little slow. I think this is because it depends on a lot of `tidyverse` functions.  While more readable, it can be sped up by several orders of magnitude if it is rewritten in data.table like so:

```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('../src/R/HPC/obis_calc_indicators_improved_dt.R')
```

```{r calc_indicators as data.table, echo=TRUE, eval = FALSE}
```


### Calculate the biodiversity indices

We loop through the quadrants of the globe, aggregated as a decade. This took about 15-20 minutes for me..


```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('../src/R/HPC/join_obis_and_gbif_global.R')
```

```{r Calc bio_idx, echo=TRUE, eval = FALSE}
```

### Make h3 Grid

The global h3 grid has to be generated for the resolution you did the indexing at, in our case resolution == 5.  You can read more about this [here](https://h3geo.org/docs/core-library/restable/). This resolution means each hexagon is ~ 253 km^2, on average. We chose this after examining some sample data at resolutions 2 - 7.  We felt a resolution of 5 balanced our desire to show hi-resolution diversity metrics without discarding too much data that was too sparse to meet the requirments of the Hurlbert (AKA ES) index). For perspective, at this resolution, the Great Barrier Reef contains about 1375 hexagons and the island of Barbados would be covered by about 2 hexagons.  

One important thing to keep in mind is that you only need to generate this grid once. Then you can stash it as an `rds` object for any subsequent runs.  Each increase in resolution corresponds to about an order of magnitude increase in the number of cells.  I noticed a real slowdown above 5. Using `future` and `furrr` to parallelize it helped some.

```{r make h3 grid, eval = FALSE}

make_h3_grids_parallel <- function (hex_res = 2){
  CRS <- sf::st_crs(4326)
  east <- sf::st_sf(geom = sf::st_as_sfc(sf::st_bbox(
    c(
      xmin = 0,
      xmax = 180,
      ymin = -90,
      ymax = 90
    ), crs = CRS
  )))
  west <- sf::st_sf(geom = sf::st_as_sfc(sf::st_bbox(
    c(
      xmin = -180,
      xmax = 0,
      ymin = -90,
      ymax = 90
    ), crs = CRS
  )))
  hex_ids <-
    c(h3::polyfill(east, res = hex_res),
      h3::polyfill(west,
                   res = hex_res))
  dl_offset <- 60
  
  future::plan(strategy = "multisession",
               workers = future::availableCores() * 0.75)
  
  #make sliding chunk size with res
  chunk <- dplyr::case_when(hex_res == 4 ~ 2e4,
                            hex_res == 5 ~ 5e4,
                            hex_res == 6 ~ 1e5,
                            TRUE ~ 1e4)
  n <- length(hex_ids)
  r  <- rep(1:ceiling(n / chunk), each = chunk)[1:n]
  hex_ids <- split(hex_ids, r)
  
  hex_sf <-
    furrr::future_map_dfr(hex_ids, function(x) {
      h3::h3_to_geo_boundary_sf(unlist(x))
    },
    .options = furrr::furrr_options(seed = NULL)) %>%
    sf::st_as_sf(data.table::rbindlist(.)) %>%
    sf::st_wrap_dateline(c(
      "WRAPDATELINE=YES",
      glue::glue("DATELINEOFFSET={dl_offset}")
    )) %>%
    dplyr::rename(hex_ids = h3_index)
  
  future::plan(strategy = "sequential")
  
  return(hex_sf)
}

h3_grids <- make_h3_grids_parallel(hex_res = 5)

```

### Join the indices and grid.

```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('../src/R/HPC/join_obis_and_gbif_global.R')
```

```{r join with h3 grid, echo=TRUE, eval = FALSE}
```

## Visualize the results. 

I used a custom function base off [something Matt Biddle wrote](https://github.com/MathewBiddle/globe/blob/main/use_parquet_file_for_globe.Rmd), and modified it with some elements from some of [John Waller's plots](https://github.com/jhnwllr/es50) so I could play with a few key parameters.

```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('../src/R/HPC/gmap_discrete.R')
```

```{r gmap discrete function, echo=TRUE, eval = FALSE}
```


Make the map. Generating and/or saving the map can take about 2-3 minutes if you increase the resolution.

```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('../src/R/HPC/join_obis_and_gbif_global.R')
```

```{r Make map, echo=TRUE, eval = FALSE}
```

*Note: I wasn't happy with how these were rendering in the notebook, so I quickly cropped some whitespace off the sides via Windows.*

| ES100 - All kingdoms, 2010s |
|----|
|<img src="../output/tests/HPC/ES100_global_2010s_res5_OBIS+GBIF_All_Kingdoms_2023-01-13.png" alt="Global ES100 
using GBIF + OBIS for the 2010s" width="100%"/>|

| ES50 - All kingdoms, 2010s |
|----|
|<img src="../output/tests/HPC/ES50_global_2010s_res5_OBIS+GBIF_All_Kingdoms_2023-01-13.png" alt="Global ES50 using GBIF + OBIS for the 2010s" width="100%"/>|

| ES50 - Animal kingdom only, 2010s |
|----|
|<img src="../output/tests/HPC/ES50_global_2010s_res5_OBIS+GBIF_Animalia_2023-01-13.png" alt="Global ES50 using GBIF + OBIS for the 2010s - Animal kingdom only" width="100%"/>|

| ES50 - Plant kingdom only, 2010s |
|----|
|<img src="../output/tests/HPC/ES50_global_2010s_res5_OBIS+GBIF_Plantae_2023-01-13.png" alt="Global ES50 using GBIF + OBIS for the 2010s - plant Kingdom only" width="100%"/>|
